{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mendefiniskan Environment\n",
    "\n",
    "Environment terdiri dari **States**, **Actions**, dan **Rewards**. States dan Actions adalah input untuk agent Q-learning, sedangkan tindakan yang mungkin terjadi adalah output agent.\n",
    "\n",
    "#### States\n",
    "States di dalam lingkungan adalah semua kemungkinan lokasi di dalam gudang. Beberapa lokasi digunakan untuk menyimpan barang (**kotak hitam**), sementara lokasi yang lain adalah lorong yang digunakan robot untuk berjalan (**kotak putih**). Kotak hijau menunjukkan area pengemasan dan pengiriman barang (**goals**).\n",
    "\n",
    "Kotak hitam dan kotak hijau adalah **terminal states**\n",
    "\n",
    "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map.png)\n",
    "\n",
    "Tujuan agent adalah mempelajari jalur terpendek antara area pengemasan barang (kotak hijau) dan semua lokasi lain di gudang tempat robot diizinkan untuk bepergian (kotak putih).\n",
    "\n",
    "Seperti yang ditunjukkan gambar di atas, ada 121 kemungkinan states (lokasi) di dalam gudang. States disusun dalam kisi yang berisi 11 baris dan 11 kolom. Setiap lokasi dapat diidentifikasi oleh indeks baris dan kolom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mendefinisikan bentuk environment yang memiliki 11 rows, dan 11 columns, sehingga memiliki 121 kemungkinan states (lokasi)\n",
    "env_rows = 11\n",
    "env_columns = 11\n",
    "\n",
    "\"\"\"\n",
    "Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a) \n",
    "The array contains 11 rows and 11 columns (to match the shape of the environment), as well as a third \"action\" dimension.\n",
    "The \"action\" dimension consists of 4 layers that will allow us to keep track of the Q-values for each possible action in each state (see next cell for a description of possible actions). \n",
    "The value of each (state, action) pair is initialized to 0.\n",
    "\"\"\"\n",
    "q_values = np.zeros((env_rows, env_columns, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions ###\n",
    "Actions yang tersedia untuk agent adalah bergerak ke empat arah:\n",
    "* Atas\n",
    "* Kanan\n",
    "* Bawah\n",
    "* Kiri\n",
    "\n",
    "Agent juga harus belajar untuk menghindari lokasi penyimpanan barang (kotak hitam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mendefinisikan action\n",
    "#up = 1, right = 2, down = 3, left = 4\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards ###\n",
    "\n",
    "Komponen terakhir dari environment adalah mendefinisikan **Rewards**.\n",
    "\n",
    "Untuk membantu agent belajar, setiap **States** (lokasi) diberi nilai **Reward**\n",
    "\n",
    "Agent bisa saja memulai dari kotak putih manapun, namun tujuannya selalu sama yaitu **memaksimalkan total rewards**\n",
    "\n",
    "**Negative rewards** (punishment) digunakan pada semua states kecual goal\n",
    "* Hal ini akan mendorong AI untuk mengidentifikasi jalur terpendek ke tujuannya dengan meminimalkan punishment.\n",
    "\n",
    "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map-rewards.png)\n",
    "\n",
    "Untuk memaksimalkan cumulative rewards (dengan meminimalkan cumulative punishment) Agent perlu menemukan jalur terpendek antara area pengemasan barang (**kotak hijau**) dan semua lokasi lain di gudang tempat robot diizinkan untuk bepergian (kotak putih). Agent juga perlu belajar untuk menghindari menabrak lokasi penyimpanan (**kotak hitam**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membuat array 2 dimensi untuk menapung reward\n",
    "#array berisi 11 baris dan 11 kolom (sesuai dengan environment), dan diberi nilai -100\n",
    "rewards = np.full((env_rows, env_columns), -100.)\n",
    "rewards [0, 5] = 100. #memberi nilai reward 100 pada goal\n",
    "\n",
    "#menentukan lorong yang akan diberi reward -1\n",
    "aisles = {} #store locations in a dictionary\n",
    "aisles[1] = [i for i in range (1, 10)]\n",
    "aisles[2] = [1, 7, 9]\n",
    "aisles[3] = [i for i in range(1, 9)]\n",
    "aisles[3].append(9) \n",
    "aisles[4] = [3, 7]\n",
    "aisles[5] = [i for i in range (11)]\n",
    "aisles[6] = [5]\n",
    "aisles[7] = [i for i in range(1, 10)]\n",
    "aisles[8] = [3, 7]\n",
    "aisles[9] = [i for i in range(11)]\n",
    "\n",
    "#nested for untuk memberi nilai reward untuk tiap lorong\n",
    "for row_index in range(1, 10):\n",
    "    for column_index in aisles[row_index]:\n",
    "        rewards[row_index, column_index] = -1\n",
    "\n",
    "#menampilkan reward\n",
    "for row in rewards:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melatih model ###\n",
    "\n",
    "Langkah selanjutnya adalah agar agent mempelajari lingkungannya dengan menerapkan model Q-learning. proses pembelajaran akan mengikuti langkah-langkah berikut:\n",
    "\n",
    "1. Pilih non-terminal state (kotak putih) secara acak untuk agent memulai episode\n",
    "2. pilih actions (bergerak ke atas, kanan, bawah, kiri) untuk state saat ini. Tindakan akan dipilih menggunakan epsilon greedy algorithm. Algoritma ini akan memilih tindakan yang paling menguntungkan untuk agent, tapi terkadang akan memilih opsi kurang menguntungkan untuk mendorong agent mengeksplor environtment.\n",
    "3. Melakukan tindakan yang dipilih, transisi ke state selanjutnya (pindah ke lokasi berikutnya).\n",
    "4. Mendapatkan reward setelah pindah ke state (lokasi) baru, dan menghitung temporal difference\n",
    "5. Memperbarui Q-value untuk state dan action sebelumnya.\n",
    "6. Jika state baru (saat ini) adalah terminal-state (kotak hitam/hijau) maka masuk ke step 1, jika bukan maka masuk ke step 2.\n",
    "\n",
    "Seluruh proses akan diulang sebanyak 1000 episode. Ini akan memberi agent kesempatan untuk mempelajari jalur terpendek antara area pengemasan barang (**kotak hijau**) dan semua lokasi lain di gudang tempat robot diizinkan bepergian (**kotak putih**), sekaligus menghindari lokasi penyimpanan barang (**kotak hitam**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#non terminal state = kotak putih/yang nilainya -1\n",
    "#terminal state = kotak hitam/yang nilaainya -100 & goal yang nilainya 100\n",
    "\n",
    "#fungsi untuk menentukan menentukan apakah lokasi yang ditentukan adalah terminal state\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "    if rewards[current_row_index, current_column_index] == -1.: #jika reward = -1 maka bukan terminal state\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "#fungsi menentukan lokasi awal secara acak (yang non terminal state/kotak putih)\n",
    "def get_starting_location():\n",
    "\n",
    "    #mendapatkan baris acak\n",
    "    current_row_index = np.random.randint(env_rows)\n",
    "    current_column_index = np.random.randint(env_columns)\n",
    "\n",
    "    #terus memilih baris dan kolom secara acak sampai ketemu non terminal state/kotak putih\n",
    "    while is_terminal_state(current_row_index, current_column_index):\n",
    "        current_row_index = np.random.randint(env_rows)\n",
    "        current_column_index = np.random.randint(env_columns)\n",
    "    return current_row_index, current_column_index\n",
    "\n",
    "#mendefinisikan epsilon greedy algorithm\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    else:#10% nya memilih action secara random (untuk eksplorasi)\n",
    "        return np.random.randint(4)\n",
    "\n",
    "#mendefinisikan lokasi berikutnya setelah melakukan action yang dipilih\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 'right' and current_column_index < env_columns -1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 'down' and current_row_index < env_rows -1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "    return new_row_index, new_column_index\n",
    "\n",
    "#fungsi untuk menentukan jalur terpendek yang dapat dilalui dari lokasi manapun yang diizinkan (lokasi yang kotak putih)\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "\n",
    "    if is_terminal_state(start_row_index, start_column_index):\n",
    "        return []\n",
    "    else:\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "\n",
    "        while not is_terminal_state(current_row_index, current_column_index):\n",
    "            action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "\n",
    "            current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "            \n",
    "        return shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training agent dengan menggunakan Q-learning ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mendefiniskan training parameter\n",
    "epsilon = 0.9 \n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.9\n",
    "\n",
    "#menjalankan sebanyak 1000 episode\n",
    "for episode in range (1000):\n",
    "\n",
    "    #mendapatkan lokasi awal untuk episode ini\n",
    "    row_index, column_index = get_starting_location()\n",
    "\n",
    "    #terus melakukan action sampai mencapai terminal state (kotak hitam/hijau)\n",
    "    while not is_terminal_state(row_index, column_index):\n",
    "        action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "        #menentukan aksi dan memilih keadaan selanjutnya (berpindah ke lokasi selanjutnya)\n",
    "        old_row_index, old_column_index = row_index, column_index\n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "        \n",
    "        #mendapatkan hadiah dan menghitung temporal difference\n",
    "        reward = rewards[row_index, column_index]\n",
    "        old_q_values = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_values\n",
    "        \n",
    "        #update nilai q\n",
    "        new_q_value = old_q_values + (learning_rate * temporal_difference)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mendapatkan Jalur Terpendek ###\n",
    "\n",
    "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_shortest_path(9, 3)) #dimulai pada baris ke 9, kolom 3\n",
    "print(get_shortest_path(5, 0)) #dimulai dari baris ke 5, kolom 0\n",
    "print(get_shortest_path(9, 5)) #dimulai dari baris ke 9, kolom 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dibalik\n",
    "path = get_shortest_path(5, 2)\n",
    "path.reverse()\n",
    "print(\"\")\n",
    "print(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('QLearning-tes': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "193d2c9c9d47fa820533aea5f8a16671999948df879bb4abe81161f8a9331358"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
